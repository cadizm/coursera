
### Debugging a learning algorithm

  - get more training examples
  - try smaller sets of features
  - try getting additional features
  - try adding polynomial features
  - try decreasing lambda
  - try increasing lambda


### Evaluating hypothesis

split dataset into sub training set and test set
typical split ratio is 70% and 30% respectively

#### Regularization parameter lambda

large lambda
  - high bias (underfit)

intermediate lambda
  - "just right"

small lambda
  - high variance (overfit)

#### Getting more training data helpful

  - when algorithm suffering from high variance
  - cross validation error much larger than training error

### Debugging a learning algorithm

  - get more training examples       => fix high variance
  - try smaller sets of features     => fix high variance
  - try getting additional features  => fix high bias
  - try adding polynomial features   => fix high bias
  - try decreasing lambda            => fix high bias
  - try increasing lambda            => fix high variance

#### Neural network example

Given a neural network with one hidden layer, you find that cross
validation error is much larger than training error. Is it likely
that increasing the number of hidden units will help?

Answer: No, because it is suffering from high variance, so adding
hidden units is unlikely to help.

Machine Learning System Design
==============================

### Building a spam classifier

The recommended approach to perform error analysis is to use cross-
validation data rather than test data because if we develop new
features by examining the test set, then we may end up choosing
features that work well specifically for the test set and `J_test(theta)`
is no longer a good estimate of how well ge can generalize to new
examples.

### Error metrics for skewed classes

```
            Actual
Predicted   1               0

1           true           false
            positive       positive

0           false          true
            negative       negative
```

Accuracy: (true pos + true neg) / total examples

Precision: true positives divided by # of predicted positives
This is the same as true positives divided by (true pos + false pos)

Recall: true positives divided by # of actual positives
This is the same as true positives divided by (true pos + false neg)

By convention, y = 1 in presence of rare class we want to detect.
So that even if we have skewed classes, the algorithm cannot "cheat"
precision and recall.

#### Trading off precision and recall

* Predict y=1 only if very confident
** higher precision, lower recall

* Avoid false negatives
** higher recall, lower precision

#### How to compare precision/recall numbers?

Average: (P + R) / 2   (usually not good)

F(1) Score: 2 * (P * R / (P + R))

#### Data for machine learning

It's not who has the best algorithm that wins. It's who has the
most data.

When is this true?

By using an algorithm with many parameters, we usually will have
low bias.

And by using a very large training set, we will usually have a
low variance (unlikely to overfit).

Having a large training set can significantly help improve a
learning algorithm's performance except when the features x, do
not contain enough information to accurately predict y.
