
Classification and Representation
=================================

## Classification
  * email: spam/not spam?
  * online transactions: fraudulent (yes/no)?
  * tumor: malignant/benign?

- y in {0, 1}
- y in {0, 1, 2, 3}

### Logistic Regression

Sigmoid (aka logistic) function

`h-theta(x) = 1 / 1 + e**((-theta)' * x)`

If we predict y = 1 if h(x) >= 0.5, then
we predict y = 1 when theta' >= 0

#### Simplified Cost Function and Gradient Descent

#### One-vs-All Classification

### Regularization

#### The Problem of Overfitting

If we have too many features, the learned hypothesis may fit the training
set very well, but fail to generalize to new examples (predict on new
examples)

##### Addressing overfitting

1. Reduce number of features
2. Regularization
   - keep all features, but reduce magnitude/values of params theta[j]
   - works well with many features

Regularization gives a "simpler" hypothesis and is less prone to overfitting.

Since we don't know in advance which features are less relevant, we modify the
cost function to add a regularization term which shrinks all parameters.

Regularization parameter lambda:
  1. controls trade off between goals fitting data and keep parameters small

If lambda is set too large, the algorithm may result in underfitting, which fails
to fit even the training set.

#### Regularized Linear Regression
