
Support Vector Machines
=======================

Relationship between C and lambda.

C does not equal 1/lambda, but given the minimizaiton objective function, they will
yield the same optimal values of theta if C = 1/lambda.

### SVM Decision Boundary

Whenever y(i) = 1:
  - we want theta' * x(i) >= 1

Whenever y(i) = 0:
  - we want theta' * x(i) <= -1


### SVM Parameters:

C (= 1 / lambda)

Large C: lower bias, high variance (overfit)

Small C: higher bias, low variance

C and sigma^2 are inversely related.

Large sigma^2: features vary more smoothly
  - higher bias, lower variance

Small sigma^2: features vary less smoothly
  - lower bias, higher variance

### Using an SVM

Use SVM software package (liblinear, libsvm, etc)

Neet to specify:
  - choice of parameter C
  - choice of kernel
      - no kernel (linear kernel)
      - gaussian kernel, need to choose sigma^2

Logistic regression vs SVM's

n = number of features
m = number of training examples

If n is large relative to m, use logistic regression, or SVM w/ no kernel
(i.e. linear kernel).

If n is small, m in intermediate, use SVM with Gaussian kernel.

If n is small, m is large, create/add more features, then use logistic
regression or SVM w/ no kernel.

Neural network likely to work well for most of these things, but may be
slower to train.
