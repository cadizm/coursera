
Unsupervised Learning
=====================

### Clustering

Applications: market segmentation, social network analysis, organizing
computing clusters, astronomical data analysis


#### K-Means algorithm

c(i): index of cluster (1, 2, ... K) to which example x(i) is currently
      assigned

mu(k): Cluster centroids

mu-c(i): cluster centroid of cluster to which example x(i) has been
         assigned

Input:
  - K (number of clustes)
  - training set (x(1), x(2), ..., x(m))
    x(i) in Rn


#### Optimization objective

J((c1), ..., c(m), mu(1), ..., mu(K)) =

    1/m * summation(norm(x(i) - m-c(i))^2)


#### Random initialization

- Should have K < m

- Randomly pick K training examples

- Set mu(1), ..., mu(K) equal to these training examples

K-means can arrive at local optima (of "distortion function" J) depening
upon random initialization

Try multiple random initializations in in multiple runs of K-means in order
to try to avlid local optima (usually for smaller values of K, e.g. 2-10)

Then pick the clustering that gave the lowest cost/distortion


#### Choosing the number of clusters

Most common is choosing the number of clusters by hand, after examining
data via visualizations, etc.

There isn't always a "correct" number, as it may be ambiguous/subjective
how many clusters there are

Methods:

Elbow Method: find "elbow" K, where after distoration decreases rapidly, it
then decreases very slowly

Elbow worth a shot, but not commonly used since it is often difficult to
identify elbow


### Dimensionality Reduction

Motivations:
  - Data compression (for example project k features onto n features, k < n)
  - Data visualization (typically to 2 or 3 dimensions)


#### Principal Component Analysis problem formulation

Find direction vectors u(1), ...u(k) in Rn onto which to project the data so as to
minimize the projection error


#### PCA algorithm

1. Data preprocessing

     - feature scaling
     - mean normalization

2. Compute covariance matrix (sigma)

     sigma = (1 / m) * X' * X;  % vectorized implementation

     x(i) is nx1, x(i)' is 1xn

3. Compute eigenvectors of covariance matrix sigma

     [u, s, v] = svd(sigma)  % singular value decomposition
     u_reduce = u(:, 1:k)
     z = u_reduce' * x

     u, s, v are nxn matrices
     first k columns of u matrix will give us the k vectors we are projecting onto


#### Applying PCA

Reconstruction from compressed representation (e.g. 1D back to 2D)

    x-approx = u-reduce * z


Choosing k, number of principal component

typically choose k to be smallest value so that "99% of variance is retained"

    [u, s, v] = svd(sigma)

    then pick smallest value of k for which

        summation(i=1 to k of s(ii)) / summation(i=1 to k of s(ii)) >= 0.99


Advice for applying PCA

Supervised learning speedup

  1. extract inputs to get unlabeled dataset
  2. apply PCA to get reduced feature set

PCA's mapping should only be run on training set


Don't use PCA to prevent overfitting. Bad idea, but not a good idea. Use
regularization instead. Reason is that it does not look at labels Y

Good/recommended applications include:

  1. compressing data to take up less memory/disk
  2. reduce dimension of input so as to spped up a learning algorithm
  3. visualize high-dimensional data by choosing k=2 or k=3
