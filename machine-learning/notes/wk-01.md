
Introduction
============


Supervised Learning
-------------------

[Supervised learning](https://en.wikipedia.org/wiki/Supervised_learning)
[Regression analysis](https://en.wikipedia.org/wiki/Regression_analysis)
[Statistical classification](https://en.wikipedia.org/wiki/Statistical_classification)
[Support vector machine](https://en.wikipedia.org/wiki/Support_vector_machine)

In _supervised learning_, you are given right answer for each sample in the data,
aka training set.

* m = number of traning examples
* x = input variables/features
* y = output variables/targets
* (x, y) = single training example


Unsupervised Learning
---------------------

[Unsupervised learning](https://en.wikipedia.org/wiki/Unsupervised_learning)
[Cluster analysis](https://en.wikipedia.org/wiki/Cluster_analysis)
[Cocktail party effect](https://en.wikipedia.org/wiki/Cocktail_party_effect)


### Linear Regression with One Variable

Recall that in *regression problems*, we are taking input variables and trying
to map the output onto a *continuous* expected result function.

Linear regression with one variable is also known as "univariate linear
regression."

Univariate linear regression is used when you want to predict a single output
value from a single input value. We're doing supervised learning here, so
that means we already have an idea what the input/output cause and effect
should be.

In a _regression problem_, you predict real-valued output.
In a _classification problem_, you predict discrete valued output.

### Model and Cost Function

For linear regression problems, the squared-error cost function is a very common
cost function which tries to fit a straight line to our data.

For fixed theta1, the hypothesis function is a function of x.

The cost function is a fuction of theta1.

#### Contour plots

[Countour plot](http://www.itl.nist.gov/div898/handbook/eda/section3/contour.htm)


#### Gradient Descent

[Gradient descent](https://en.wikipedia.org/wiki/Gradient_descent)

parameter alpha: learning rate (how big of a step we take)


### Linear Algebra Review

#### Properties

* matrix multiplication *is not* commutative
* matrix multiplication *is* associative
* Given an nxn identify matrix I[nxn],

```
[1 0]
[0 1]

[1 0 0]
[0 1 0]
[0 0 1]
```

For any matrix A, A * I = I * A = A

#### Inverse and Transpose

If A is an *mxm* matrix, and if it has an inverse,

A * inv(A) = inv(A) * A = I

Matrices that do not have an inverse are "singular" or "degenerate".

```
A = [1 2 0]
    [3 5 9]

transpose(A) = [1 3]
               [2 5]
               [0 9]
```

Let A be an mxn matrix, and let B = transpose(A),
Then B is an nxm matrix, and B[i,j] = A[j,i]
