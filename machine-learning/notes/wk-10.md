
Large Scale Machine Learning
============================


### Learning with Large Datasets

Take low bias algorithm and train with alot of data.

Given m = 100,000,000 and doing gradient descent, step of gradient descent
requires computing the sum over m to compute the derivative term.

A way sanity check whether using a small subset of entire dataset suffices
is by plotting a learning curve.

When you have low bias, then the algorithm will perform better given more
training examples. With high bias, more examples may not be necessary.
With high bias add more features/nodes in neural network, to increase
confidence that a small dataset suffices.


### Stochastic Gradient Descent

1. Randomly shuffle dataset

2. Repeat {
    for i  1, ...m, {
        for j = 0, ..., n {
            % make an update given a single example
            theta-j = theta-j - alpha h-theta(x(i) - y(i) * x(j))
        }
    }
}

Stochastic gradient descent does not actually "converge", but somewhere
"close" to global minimum.


### Mini-Batch Gradient Descent

Batch gradient descent: use all m examples in each iteration

Stochastic gradient descent: use 1 example in each iteration

Mini-Batch gradient descent: use b examples in each iteration

b = mini-batch size

We look at b examples instead of 1 because we can vectorize in implementation.


### Stochastic Gradient Descent Convergence

Every 1000 iterations or so, plot cost(theta, x(i), y(i)) averaged over
the last 1000 examples processed by algorithm.

Plots allow us to check if algorithm is converging.


Online Learning
===============

Look at one example at a time, learn from it, then discard and move on (can
adapt to change in user preferences)


Map Reduce and Data Parallelism
===============================

When bulk of algorithm's work can be expressed as a sum over training set,
map reduce may be a good option
